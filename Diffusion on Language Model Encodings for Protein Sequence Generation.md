# Diffusion on Language Model Encodings for Protein Sequence Generation

# 摘要

蛋白质序列设计近年来通过离散扩散模型和自回归方法取得了显著进展，然而连续扩散模型的潜力仍未得到充分探索。在此，我们提出了 **DiMA**——一种在蛋白质语言模型表示空间中运行的**隐式扩散框架**。通过对架构设计与扩散组件的系统性探索，我们开发出一种鲁棒的方法论，可通用于多种蛋白质编码器，参数规模涵盖从 800 万到 30 亿。

我们证明，该框架在仅使用序列信息（如 ESM2、ESMc）、双解码结构（如 CHEAP）以及多模态表示（如 SaProt）的情况下，均能以**相同的架构和训练策略**实现 consistently 高性能表现。

我们对包括 DiMA 在内的现有方法进行了全面评估，覆盖两种蛋白质模态，采用多种指标衡量生成蛋白质的**质量、多样性、新颖性以及与真实分布的匹配程度**。结果表明，DiMA 能够持续生成**新颖、高质量且多样化的蛋白质序列**，在各项指标上均显著优于自回归模型、离散扩散模型以及流匹配语言模型等基线方法。

此外，该模型展现出强大的多功能性，支持多种**条件生成任务**，包括：  
- 蛋白质家族定向生成  
- 功能基序（motif）支架设计  
- 序列片段填充（infilling）  
- 特定折叠结构（fold-specific）的序列设计  

本工作提供了一个**通用的连续扩散框架**用于蛋白质序列生成，不仅为架构设计提供了深刻见解，也在多种蛋白质设计场景中展现出实际应用价值。

# 1. 背景

蛋白质生成是当前生物计算、合成生物学和药物研发的核心方向。当前主流方法包括：

- **自回归模型**：擅长捕捉序列依赖性。
- **扩散模型**（尤其是离散扩散）：用于氨基酸序列生成效果良好。
- **基于流的模型**：在条件结构生成方面表现突出。

近年来，**蛋白质语言模型（pLMs）** 发展迅速，产生高质量、连续的蛋白质表示（如 ESM、CHEAP、SaProt），能同时编码序列与结构信息。**连续扩散**（continuous diffusion）在图像/语音等领域很成功，但在蛋白质生成领域仍处于早期阶段。

现有研究存在的问题：

1. **连续扩散未被充分探索**
   - 以往工作多集中于**离散扩散**或**三维结构扩散**。
   - 连续扩散在蛋白质序列生成中的潜力被忽视。
2. **现有连续扩散方法缺乏通用性**
   - 多数尝试局限于**特定蛋白质表示**或**特定条件任务**（如仅做 motif 设计）。
   - 无法泛化到不同编码器或通用生成任务。
3. **离散建模存在固有缺陷**
   - 序列是离散符号，建模困难（如长程依赖、组合爆炸）。
   - 连续表示（如 pLM 嵌入）已被证明在重建精度上优于离散表示。

<img src="./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020135732245.png" alt="image-20251020135732245" style="zoom:150%;" />

本文提出 **DiMA（Diffusion Model on protein representations with Agnostic encoder）**：

- 一种**连续隐式扩散模型**，直接作用于蛋白质语言模型（pLM）的**连续嵌入空间**。
- 核心思想：**不在原始氨基酸序列上扩散，而在 pLM 提取的连续向量空间中扩散**。
- 架构设计为**编码器无关**（encoder-agnostic），可适配多种 pLM 编码器（ESM-2, CHEAP, SaProt 等）。
- 通过系统性探索架构和扩散组件，构建**鲁棒、可泛化的框架**。

# 2. 蛋白质序列的LM表示上的连续扩散

DiMA是一种在连续蛋白质表示上运行的潜在扩散模型。DiMA由三个组件构成：一个预训练编码器 $(\mathcal{E})$，用于提供有意义的潜在空间表示；一个扩散模型 $(\mathcal{F})$，用于从高斯噪声中生成潜在向量；以及一个解码器 $(\mathcal{D})$，用于将生成的潜在向量映射回氨基酸序列。

## 蛋白质的连续扩散与离散扩散

尽管离散扩散在序列生成方面可能看起来更直观，但连续表示为蛋白质建模提供了引人注目的优势。来自蛋白质语言模型的连续编码捕捉了丰富的语义和结构信息，在从表示学习到结构预测等各类蛋白质任务中已被证明有效。最近的研究表明，与离散替代方法相比，连续表示具有更高的重建精度，暗示了其在生成建模方面的潜力。

连续扩散相较于离散方法具有若干理论和实践优势，例如可直接应用已建立的基于分数的技术，如分类器和无分类器引导，而无需进行离散近似。它还提供了与多模态表示（如 CHEAP、SaProt）的无缝集成，这些表示共同编码序列和结构。此外，连续扩散在训练中比离散扩散更具可扩展性和效率。然而，蛋白质序列固有的顺序性和离散性为连续扩散带来了独特的挑战，这需要对扩散组件进行仔细调整，以有效捕捉蛋白质空间的复杂性。

## 潜在空间适配

我们使用一个预训练的基于 Transformer 的蛋白质语言模型（pLM）作为编码器，除非另有说明，默认选择 ESM-2。该编码器将长度为 $s$ 的离散氨基酸序列 $y = [y_1, ..., y_s]$ 映射到潜在向量 $x = [x_1, ..., x_s] \in \mathbb{R}^{s \times d}$，即 $x = \mathcal{E}(y)$。我们在隐藏维度 $d$ 上对 $x$ 应用归一化 $z_0 = \text{Normalize}(x)$：对每个分量 $d_i$，我们先在训练数据上计算其均值和方差，然后利用这些统计量进行归一化，以实现零均值和单位方差。这种变换使我们能够将离散的蛋白质输入适配到标准高斯扩散框架中。

## 噪声调度优化

我们发现，在图像领域广泛使用的线性与余弦噪声调度器对于蛋白质域而言是次优的。我们推测这是由于蛋白质表示的序列性和离散性所致。

使用此类调度器训练的扩散模型，其重建损失在小噪声尺度下保持最小。因此，从 $z_t = \sqrt{\alpha_t} z_0 + \sqrt{1 - \alpha_t} \epsilon$ 中重建 $z_0$ 对模型而言在很长一段时间内变得相当简单，导致训练效率低下。我们采用了来自（Hoogeboom 等，2023）的噪声调度：

$$
\alpha_t = \frac{1}{1 + d^2 \tan^2(\frac{\pi t}{2})}
$$

其中 $d$ 是一个超参数，反映调度速率。$d$ 的值越大，数据损坏率越高。在本工作中，我们使用 $d=10$，因此该调度被称为 **tan-10**。我们采用了一种启发式方法，基于以下观察：重建损失应随扩散时间近似线性增长（图2）。

<img src="./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020105503899.png" alt="image-20251020105503899" style="zoom:150%;" />

**C.5 tan-10 调度背后的原理**

标准噪声调度器（线性、余弦）的关键问题在于，它们在较小的时间步长上对数据的破坏非常缓慢（见图2）。这意味着模型会将大量训练资源耗费在近乎平凡的去噪任务上——此时输入与目标几乎完全相同。我们的方法确保了去噪任务的难度随每个时间步稳步增加。通过设计一种重建损失随扩散时间线性增长的噪声调度器，我们创建了难度逐步递增的学习问题，使模型在整个训练过程中能够保持稳定进步，而非面临任务复杂度的突然跳跃。

![image-20251020110920161](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020110920161.png)

当处理蛋白质编码时，我们发现小噪声水平下的重建表现相当鲁棒。例如，在使用 CHEAP 表示（支持序列与结构的双重解码）测试 DIMA 时，我们观察到在 $t = 0.05$ 时，序列重建准确率仍保持为 100%，且结构 RMSD 低于 $0.2\,\text{Å}$（见图13和图14）。这表明，与基于像素的图像扩散模型不同，DIMA 并不需要在小噪声阶段投入过多努力来学习精细的去噪能力。我们的调度器利用了这种鲁棒性，将更多训练资源分配给更具挑战性的噪声水平，而非接近无损的 $t \to 0$ 阶段。

## 自条件化

受序列生成领域最新进展的启发，我们应用了自条件化技术。去噪网络以潜在变量 $z_t$ 和时间步 $t$ 作为输入，预测 $\hat{z}_0$。自条件化还额外利用前一时间步 $s$ 预测的 $\hat{z}_{0,s}$ 来估计当前时间步的 $\hat{z}_{0,t} = \hat{z}_\theta(z_t, t, \hat{z}_{0,s})$，其中 $t < s$。

在训练过程中，我们对时间步 $t$ 从 $U[0;1]$ 中采样。在一半的情况下，我们不向模型提供额外输入，将 $\hat{z}_{0,t}$ 设为零向量 $\emptyset$；在其余情况下，我们估计 $\hat{z}_{0,t} = \hat{z}_\theta(z_t, t, \emptyset)$。损失函数计算如下：

$$
\mathcal{L}(\theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1),\, t \sim U[0;1]} \left[ \| z_0 - \hat{z}_\theta(z_t, t, \text{SG}[\hat{z}_{0,t}]) \|^2 \right]
$$

其中，$\text{SG}[\cdot]$ 表示停止梯度操作。与（Chen 等，2022）不同，我们对 $\hat{z}_{0,t}$ 应用线性变换，并将其融入每个 Transformer 块的输入中。自条件化比率对质量-多样性权衡的影响如图12所示。

## Token 重建

我们的架构使用了与编码器一同在掩码语言建模目标下预训练的 ESM-2 解码器。我们发现，对解码器进行额外的微调——特别是针对氨基酸重建任务——可以提高从潜在表示 $x$ 在推理阶段生成序列的准确性。该解码器在推理时保持简单的架构，仅包含一个线性层。对于 CHEAP 和 SaProt 表示，我们采用类似 ESM-2 的方法，沿着扩散模型一起微调其对应的预训练解码器。我们发现，低维嵌入（例如，ESM-2 8M, d=320）相比高维嵌入（ESM2/SaProt 650M, d=1280；CHEAP, d=1024）对微小扰动更不鲁棒。微调解码器有助于在扩散生成过程中最小化这些影响。

## 长度确定

确定序列长度是推理阶段的关键挑战。尽管许多离散扩散模型会与语义 token 一同生成填充 token，但我们发现这种方法在蛋白质生成中次优。相反，我们采用了两种不同的策略用于训练和推理。

在训练期间，我们使用注意力掩码，使模型专注于语义 token。这种掩码策略至关重要，因为特殊 token（如填充 token）的编码通常包含对扩散模型无关或潜在有害的信息。通过在重建损失计算中排除这些 token，我们提高了训练稳定性和生成质量。

在推理期间，我们首先从训练数据分布中采样目标序列长度，以确保生成的蛋白质长度符合现实。然后，我们采样一个适当维度的随机高斯向量，并应用 $T$ 步迭代细化过程来生成 $\hat{z}_0$。最后，我们对潜在表示进行去归一化处理，并将其解码为氨基酸序列。

在推理过程中，模型必须确定所生成序列的长度。我们探索了两种策略来应对这一挑战：训练包含填充掩码的扩散模型和不包含填充掩码的扩散模型。

*   **带填充掩码的扩散模型**：在训练期间，我们为填充 token 提供注意力掩码，并在计算扩散损失时排除这些填充 token。在推理时，序列长度从训练集中观察到的经验长度分布中采样。
*   **不带填充掩码的扩散模型**：在此设置下，训练期间未提供任何关于填充的显式信息，损失是在整个序列（包括填充 token）上计算的。因此，在推理时，模型必须在没有外部引导的情况下隐式推断出合适的序列长度。

![image-20251020135315277](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020135315277.png)

图10比较了来自训练集的序列与使用第二种方法生成的序列的长度分布。值得注意的是，生成的序列表现出分布偏移，偏离了训练数据预期的长度分布。

为了缓解这种不匹配，我们采用了注意力掩码方法，在训练期间引入显式的长度信息，并在推理时采样长度，以确保与训练分布更紧密地对齐。

## 模型架构

![image-20251020140304450](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020140304450.png)

我们的扩散模型采用了一个包含 12 层、16 个注意力头、隐藏层大小为 320 的 Transformer 架构。为了将此架构适配于蛋白质序列生成任务，我们实施了两项关键修改。

为了融入时间依赖信息，我们通过线性投影将时间嵌入集成到每个 Transformer 块的输入中，随后进行求和操作。我们扩展了这一机制，以支持条件生成任务，包括特定蛋白质家族的生成、基序支架设计以及特定折叠的序列设计。为了进一步增强条件控制，我们引入了交叉注意力块来处理条件编码，使模型能够有效利用结构约束。

受 (Bao 等, 2023) 启发，我们在整个 Transformer 架构中引入了长距离跳跃连接。我们的实验表明，这些连接显著加速了模型收敛并提升了训练效率，从而促进了更稳定且高质量的蛋白质序列生成。

# 3. 实验

在本节中，我们首先识别出哪些架构组件会显著影响生成质量、多样性及分布特性（§3.2）。接着，我们证明我们的架构在不同规模的蛋白质编码器和数据集上均能保持强劲性能，无需进行任何修改（§3.4）。最后，我们通过展示其在家族特异性设计（§3.6.2）、基序支架构建（§3.6.1）以及折叠条件生成（§3.6.3）等实际应用中的表现，使用定量指标来评估各项任务的成功程度。

## 3.1 评估指标

我们采用四个核心指标全面评估生成蛋白质的质量与多样性：  
- **序列 Fréchet 距离（FD-seq）**：衡量生成序列与天然蛋白质分布的一致性；  
- **pLDDT**：评估预测结构的局部可靠性；  
- **50% 序列同一性下的聚类密度（CD₀.₅）**：量化序列多样性；  
- **新颖性得分**：检测生成样本是否过度记忆训练数据。  

此外，我们还结合结构指标（TM-score、scPerplexity）、分布距离（MMD、1-Wasserstein）及多阈值多样性分析，在 **2,048 个样本** 的大规模测试集上进行稳健评估。

## 3.2 去噪器组件分析

基于高斯扩散的现有蛋白质生成方法在选择最优方法时不够充分，很大程度上依赖于从图像扩散模型中借鉴的技术。在本研究中，我们认识到必须谨慎选择扩散组件，以便开发出能够有效捕捉蛋白质空间复杂模式的高斯扩散模型。

在本部分研究中，我们使用 ESM-8M 编码器和 DiMA-35M 模型进行实验。为了评估所提出的设计选择对 DiMA 性能的贡献，我们从零开始训练了多个模型，并进行了以下修改：移除浅层与深层 Transformer 块之间的长距离跳跃连接；通过在每个 Transformer 块之前添加专门的时间层来替代将时间嵌入与被污染的潜在向量相加的方式，实现时间条件化；省略 Transformer 编码器（ESM-2），仅保留其嵌入矩阵；在不使用自条件化的情况下训练模型；使用线性或余弦噪声调度训练模型；训练包含或不包含先验长度采样的填充重建模型；省略解码器的微调；以及在我们的潜在生成模型中采用流匹配范式。

<img src="./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020142243909.png" alt="image-20251020142243909" style="zoom:150%;" />

![image-20251020142202153](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020142202153.png)

表1表明，每一项所提出的特性都对模型性能有显著的独立贡献。在没有 ESM-2 编码器、没有长度采样以及未使用自条件化训练的消融模型中，生成序列的质量和分布相似性均出现了最显著的下降。移除跳跃连接和时间层的影响相对较小，但仍会导致生成序列重复性增加以及整体质量略有提升。

为消除 tan-10 噪声调度的影响，我们在保持其他参数不变的情况下，使用标准的线性和余弦调度重新训练了扩散模型。我们发现，tan-10 调度在质量和分布相似性方面均显著优于余弦调度。它也比线性调度取得了更少过拟合但更好的结果。



## 3.3 不同生成范式间的比较

我们在 SwissProt 上以相同规模（35M 参数）训练并对比了多种公开的蛋白质序列生成模型，包括：  
- **自回归模型**（RITA、SeqDesign、nanoGPT）  
- **基于分数的模型**（WalkJump）  
- **GAN**（ProteinGAN）  
- **离散扩散模型**（EvoDiff-OADM、DPLM、D3PM）  
- **流模型**（DFM）  

<img src="./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020142602092.png" alt="image-20251020142602092" style="zoom:150%;" />

结果（表2）表明，**DiMA（潜在高斯扩散）显著优于所有基线**：  
- 在 **分布对齐**（FD-seq）和 **结构质量**（pLDDT）上最接近真实数据；  
- **nanoGPT** 表现次优，但质量与分布匹配仍不足；  
- **DPLM** 虽具合理 pLDDT，但因序列重复和复制，**新颖性低3倍、重复率高2倍**；  
- 其他方法（如 SeqDesign、ProteinGAN、EvoDiff）整体性能较差，难以泛化至多样蛋白质空间。  

![image-20251020142647252](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020142647252.png)

### 3.3.1 生物学相关性

![image-20251020143843930](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020143843930.png)

为了探索所生成序列的生物学相关性，我们采用了成熟的蛋白质注释工具 InterProScan。我们使用了三个不同的 SwissProt 训练模型：DPLM、DiMA 和 nanoGPT。我们的分析表明，DiMA 和 DPLM 这两种表现出高质量指标的模型，与表现较差的 nanoGPT 相比，能持续生成具有高度注释性的序列（图23A）。这一模式在注释交集分析中得到了进一步体现，其中 DiMA 和 DPLM 的注释显示出更显著的重叠（图23B）。

尽管这两种方法都能生成具有相似注释水平的蛋白质，但它们在结构域长度特征上存在差异。DiMA 能准确复现数据集中的结构域长度，并倾向于生成较小的结构域（50-75个氨基酸）。相比之下，DPLM 经常产生更长的结构域（长度约254个氨基酸）（图23C）。我们假设，DPLM 中长结构域的普遍性与其较低的生成多样性相关，这一点已由我们的多样性和分布相似性指标（表6）所证实。

## 3.4 表示空间扩展

在前面的章节中，我们通过全面的消融研究建立了一个稳健的扩散模型，并展示了其相对于基线生成方法的强大性能。在此基础上，我们现在探索该模型在不同蛋白质表示空间中的适应性，使用大规模的 AFDBv4-90 数据集进行评估。

AFDBv4-90 数据集是 UniRef50 的一个精心筛选的子集，包含 220 万条蛋白质序列，其 AlphaFold2 预测的结构置信度均超过 90%。该数据集在设计上排除了内在无序蛋白和低熵序列，确保了高质量的蛋白质语料库。因此，这种筛选使我们能够将 pLDDT 作为衡量蛋白质结构质量的可靠且一致的指标。

我们保留了在消融研究（§3.2）中开发的核心去噪模型架构，仅使用简单的线性投影技术来适配不同的编码器维度。在这里，我们考察了三种具有代表性的编码器架构：

*   **ESM-2 序列专属编码器家族**：涵盖从 8M 到 3B 参数规模的模型，使我们能够研究模型复杂度对性能扩展的影响（§3.4.1）。
*   **CHEAP 表示**：其独特之处在于能从其潜在空间解码出序列和三维结构（§3.4.2）。
*   **SaProt 编码器**：它整合了来自 FoldSeek 的 3Di 词汇表中的结构标记，引入了一种融合了结构感知能力的序列表示混合方法（§3.4.3）。

通过保持核心扩散方法论不变，我们的目标是理解不同的表示空间如何影响蛋白质序列生成，从而为不同蛋白质嵌入方法的生成能力提供深入见解。

### 3.4.1 ESM-2 编码器的性能扩展

<img src="./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020145722374.png" alt="image-20251020145722374" style="zoom:150%;" />

![image-20251020145757046](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020145757046.png)

我们在 ESM-2 系列（8M 至 3B 参数）上评估 DiMA，发现生成质量（pLDDT）随模型规模稳步提升（74.3 → 83.4），且分布匹配显著改善（FD-seq 从 0.560 降至 0.314）。  

然而，**更大的编码器带来多样性下降**（CD₀.₅ 从 0.981 降至 0.969），表明存在**质量–多样性权衡**：小模型易重复，大模型更精确但新颖性受限。  

新发布的 **ESM-C 300M** 在 pLDDT（82.7）上媲美 ESM-2 650M（82.5），但覆盖范围略逊。从 650M 到 3B 的边际收益递减，暗示**中等规模模型可能更具性价比**。  

值得注意的是，DiMA 在推理时无需编码器，可轻量部署，同时兼容不同复杂度的表示空间。

### 3.4.2 适配 CHEAP 表示（精简版）

CHEAP 是一种基于 ESMFold 的双模态编码器，能从序列输入中联合压缩出高保真的序列与结构信息。我们评估了两种压缩变体：  
- **CHEAP_shorten_1**（仅降维）  
- **CHEAP_shorten_2**（降维 + 序列长度压缩）

**序列生成**：两者均表现优异，pLDDT 分别达 **81.9** 和 **78.8**（接近数据集基准 83.9），表明完整序列维度更利于生成质量。

![image-20251020151155733](./Diffusion%20on%20Language%20Model%20Encodings%20for%20Protein%20Sequence%20Generation.assets/image-20251020151155733.png)

**结构生成**：在两种协议下均表现强劲：  

- **仅结构评估**：成功率 92.3%，scRMSD 1.091 Å  
- **共设计（序列+结构联合）**：成功率 88.8%，scRMSD 1.043 Å  

值得注意的是，**CHEAP_shorten_2 的序列压缩使 Transformer 上下文减半，在几乎不损失质量的前提下显著提升计算效率**。

更多结果见附录 E.6。