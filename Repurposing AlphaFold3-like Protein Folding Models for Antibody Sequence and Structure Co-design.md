# Repurposing AlphaFold3-like Protein Folding Models for Antibody Sequence and Structure Co-design

# 1. 摘要

扩散模型在加速抗体设计方面具有巨大潜力，但其性能迄今为止受限于用于模型训练的抗体-抗原复合物数量。与此同时，类似 AlphaFold3 的蛋白质折叠模型在大量晶体结构语料库上进行了预训练，已获得对生物分子相互作用的广泛理解。基于这一洞见，我们开发了一种新的抗原条件化抗体设计模型，通过改造类似 AlphaFold3 模型的扩散模块，实现序列与结构的协同扩散（sequence-structure co-diffusion）。具体而言，我们在其结构扩散模块的基础上增加了一个序列扩散头，并对整个蛋白质折叠模型进行微调，以实现抗体序列与结构的协同设计。

我们的基准测试结果表明，序列-结构协同扩散模型不仅在性能上超越了当前最先进的抗体设计方法，而且在结构预测准确性方面与原始折叠模型相当。值得注意的是，在抗体协同设计任务中，我们的方法对典型抗体的 CDR-H3 区域实现了 65% 的恢复率，比现有基线方法高出 87%；对于纳米抗体（nanobodies），也取得了高达 63% 的恢复率。

# 2. 背景

单克隆抗体是一种 Y 形蛋白质，能够通过其互补决定区（CDRs）特异性识别并中和抗原。CDRs 的序列和结构决定了抗体的结合特异性，并具有高度多样性。近年来，生成模型在设计能与抗原结合的 CDRs 方面展现出巨大潜力，但由于抗体-抗原复合物数据稀缺，其性能仍受到限制。

为解决抗体-抗原复合物数据稀缺的问题，作者提出利用从大规模蛋白质相互作用数据中学习到的通用生物分子知识（如蛋白质折叠模型）来增强抗体设计。其核心假设是，蛋白质折叠模型（如 AlphaFold3、Chai-1、Boltz-1、Proteinix）中学到的蛋白质-蛋白质相互作用知识可以迁移到抗体-抗原建模任务。

为验证该假设，作者在开源模型 Boltz-1 的基础上引入了“序列扩散”机制，使模型能够同时生成抗体的序列与结构。具体地，模型在原有结构扩散模块中增加了一个序列扩散头，分别采用离散扩散（D3PM-absorbing）与连续扩散（EDM）两种方法，并通过对齐序列与结构扩散过程实现协同设计。

在抗体-抗原复合物数据集上微调后，实验结果表明该改进模型在抗体序列与结构的协同设计任务上优于现有基线，同时保持了 Boltz-1 的结构预测性能。

**总结来说，我们的主要贡献如下：**

*   据我们所知，我们是第一个利用类似 AF3 的折叠基础模型中的通用蛋白质相互作用知识，来有效解决抗体-抗原复合物数据有限问题的团队。
*   在类似 AF3 模型的结构扩散框架基础上，我们引入了序列扩散，并辅以定制化的训练策略，以实现有效的协同设计。
*   结果表明，我们修改后的类似 AF3 的模型在抗体序列和结构协同设计任务中优于现有基线，同时保持了其原有的结构预测能力。

# 3. 方法

在第 3.1 节，我们首先详细描述了本文所研究的任务。在第 3.2 节，我们通过回顾 AF3 的工作流程来介绍关键符号。最后，在第 3.3 节，我们详细阐述了对类似 AF3 模型进行的直接修改，这些修改使其能够支持抗体协同设计。

## 3.1 问题描述

![image-20251111150703323](C:/Users/wak/AppData/Roaming/Typora/typora-user-images/image-20251111150703323.png)

我们在图 1 中展示了一个抗体-抗原复合物的示意图。在本工作中，我们遵循抗体设计中广泛采用的设置：在实验解析的共晶体结构中重新设计 CDRs，其中所有非 CDR 区域（抗体框架序列/结构、抗原序列/结构以及抗体-抗原结合姿态）都是给定的。目标是在这个预定义的上下文中，协同设计 CDRs 的序列和结构。

特别地，与一次只设计一个 CDR 不同，我们的目标是一次性同时设计所有 CDRs，这是一个更具挑战性的任务。在本研究中，我们采用 AF3 的符号系统来表征蛋白质复合物，使用 $\{\mathbf{k}_i\}$ 和 $\{\mathbf{x}_l\}$ 分别指定复合物的氨基酸（即残基）组成和结构。$\mathbf{k}_i$ 是一个 one-hot 编码，代表每个氨基酸 token 的类型（20 种标准氨基酸类型 + `<X>`）。`<X>` token 最初在类似 AF3 的折叠模型中用于指示残基类型未知的位置，此时仅预测骨架原子坐标。$\{x_l\}$ 中的每个元素代表一个原子的三维坐标。

我们使用 `<X>` token 来表示 CDRs 中需要被设计的残基。在最终输出中，每个 `<X>` token 将被替换为 20 种标准氨基酸类型中的某一种特定残基，并附带其相应的原子坐标，从而实现抗体协同设计的目标。

## 3.2 预备知识：类似 AlphaFold3 的折叠模型

类似 AF3 的方法被构造成条件扩散模型，主要由两个组件构成：**条件主干（Conditioning Trunk）** 和 **扩散模块（Diffusion Module）**。这些模型还包含一个置信度模块，但它被独立训练，并非本文的主要关注点，因此我们不对其进行讨论。

以下内容中，带有下标 $i/j$ 的符号表示 token 级别的表示，而带有下标 $l/m$ 的符号表示原子级别的表示。在以下讨论中，**原始 AF3 论文中引入的模块被高亮显示**，相应的详细信息可参考 AF3 的补充材料。

### 条件主干

该主干处理原始输入，记为 $\{\mathbf{f}^*\}$，例如待预测的蛋白质序列、MSA 结果和结构模板信息。最初，这些输入通过一个 **InputEmbedder**，执行基本编码以获得输入特征 $\{\mathbf{s}^{\text{inputs}}_i\}$。这些初始输入特征随后通过三个模块的堆栈进行迭代更新：**TemplateModule**、**MSAModule** 和 **PairFormerStack**。最后，从主干开始时获得的输入特征 $\{\mathbf{s}^{\text{inputs}}_i\}$，连同迭代更新后的单体和成对表示，记为 $\{\mathbf{s}^{\text{trunk}}_i\}$ 和 $\{\mathbf{z}^{\text{trunk}}_{ij}\}$，被输入到下游的扩散模块，用于调节结构扩散过程。

### 结构扩散

AF3 采用 EDM 进行结构扩散训练。值得注意的是，EDM 与典型的扩散模型不同，后者在训练期间根据预定义的噪声调度表采样时间步长来确定噪声水平。相反，EDM 直接通过从定义的高斯分布中抽取噪声强度的对数来采样噪声水平。

具体而言，在 AF3 的训练过程中，噪声水平 $\sigma$ 从分布 $\sigma_{\text{data}} \cdot \exp(-1.2 + 1.5 \cdot \mathcal{N}(0,1))$ 中采样，并与来自前一主干的三个输出一起输入到 **DiffusionConditioning** 模块中。在此模块中，残基 token 之间的相对位置编码被整合进 $\{\mathbf{z}^{\text{trunk}}_{ij}\}$，以促进后续使用的 token 级别和原子级别自注意力机制 [Vaswani, 2017]，从而得到输出 $\{\mathbf{z}_{ij}\}$。此外，在此模块中，采样的噪声水平使用随机傅里叶特征 [Tancik 等, 2020] 进行嵌入，然后与 $\{\mathbf{s}^{\text{inputs}}_i\}$ 和 $\{\mathbf{s}^{\text{trunk}}_i\}$ 进行整合，以构建 $\{\mathbf{s}_i\}$。

接下来，遵循 EDM，AF3 将带噪的原子坐标 $\{\mathbf{x}^{\text{noisy}}_l\}$ 缩放到单位方差，从而得到 $\{\mathbf{r}^{\text{noisy}}_l\}$。$\{\mathbf{r}^{\text{noisy}}_l\}$、$\{\mathbf{s}^{\text{trunk}}_i\}$ 和 $\{\mathbf{z}_{ij}\}$ 在 **AtomAttentionEncoder** 模块中进一步处理。最初，该模块生成三个原子级表示：$\{\mathbf{q}_l\}$、$\{\mathbf{c}_l\}$ 和 $\{\mathbf{p}_{lm}\}$。这些表示均基于来自原始输入的原子特征，并结合了来自 $\{\mathbf{r}^{\text{noisy}}_l\}$、$\{\mathbf{s}^{\text{trunk}}_i\}$ 和 $\{\mathbf{z}_{ij}\}$ 的额外信息。然后，应用序列-局部注意力机制，利用 $\{\mathbf{c}_l\}$ 更新 $\{\mathbf{q}_l\}$。更新后的 $\{\mathbf{q}_l\}$ 随后被聚合，形成 token 级别的表示 $\{\mathbf{a}_i\}$。

接着，$\{\mathbf{s}_i\}$ 用于更新 $\{\mathbf{a}_i\}$。更新后的 $\{\mathbf{a}_i\}$，连同来自先前 **DiffusionConditioning** 模块的输出 $\{\mathbf{a}_i\}$ 和 $\{\mathbf{z}_{ij}\}$，被输入到 **DiffusionTransformer** 模块，以在 token 级别上执行全自注意力机制。该模块进一步更新 $\{\mathbf{a}_i\}$ 并应用层归一化。

token 级别的表示 $\{\mathbf{a}_i\}$ 与原子级别的表示 $\{\mathbf{q}_l\}$、$\{\mathbf{c}_l\}$ 和 $\{\mathbf{p}_{lm}\}$ 一同被输入到 **AtomAttentionDecoder** 模块，该模块输出 $\{\mathbf{r}^{\text{update}}_l\}$。$\{\mathbf{r}^{\text{update}}_l\}$ 被缩放回原尺度并与输入的带噪结构 $\{\mathbf{r}^{\text{noisy}}_l\}$ 结合，以获得最终预测的去噪结构 $\{\mathbf{x}^{\text{denoised}}_l\}$。

上述结构扩散训练过程被封装在 AF3 的 **DiffusionModule** 中，如 AF3 补充材料中的算法 20 所述。总而言之，该过程利用主干的输出，结合原始输入，进一步构建原子级别和 token 级别的表示。这些表示随后被输入到结构去噪网络中进行恢复。

为了推理，AF3 采用 EDM 随机采样器，并从一个包含 200 个递减水平的序列中定义：

$$
\left\{ \sigma_{\text{data}} \cdot \left( \sigma_{\max}^{1/\rho} + \frac{t}{200 - 1} \cdot \left( \sigma_{\min}^{1/\rho} - \sigma_{\max}^{1/\rho} \right) \right)^\rho \Bigg| t = 0, 1, \dots, 199 \right\},
$$
其中，$\sigma_\text{data}, \sigma_\text{max}, \sigma_\text{min}$ 和 $\rho$ 是超参数。

## 3.3 我们的方法

输入包含来自抗体和抗原的残基 token。对于需要被设计的抗体 CDRs，这些位置用 `<X>` token 填充。考虑到残基 token 的离散性质（作为分类变量），以及其输入格式与掩码语言模型（MLMs）（如 BERT [Devlin 等, 2019]）中使用的 `<MASK>` token 的相似性——其中 `<X>` 类似于 `<MASK>` token——D3PM-absorbing 方法在直觉上非常契合此任务。因此，我们选择使用 D3PM-absorbing 进行序列扩散。

在基于扩散的协同设计方法训练过程中，需要将序列和结构在一定程度上进行破坏，这可以通过在同一时间步采样它们来实现。然而，正如第 3.2 节所述，类似 AF3 的模型通过直接采样噪声水平来进行结构扩散训练，而不同于 D3PM-absorbing（后者使用时间步采样）。此外，据我们所知，目前尚无关于不使用采样时间步训练离散扩散的理论。因此，对齐序列和结构扩散是具有挑战性的。

### 作为序列扩散的替代方案：连续扩散

值得注意的是，Hoogeboom 等人提出了一种通过对离散数据添加高斯噪声来应用连续扩散的方法。这种方法允许序列在训练时也使用 EDM 方法进行训练。这样，序列扩散可以很容易地与 AF3 中的结构扩散训练对齐。然而，对于离散数据的生成，这种方法的效果往往不如离散扩散 [Austin 等, 2021a; Gruver 等, 2023a]，正如第 4.1 节的结果所支持的那样，我们也已实现了该方法。然而，我们的结果也揭示了它相对于结构预测的独特优势。由于本文主要关注与 AF3 相比的序列设计附加实现，且涉及离散数据，我们在下文的主要论文中集中讨论使用 D3PM-absorbing 进行序列扩散的方法。连续版本的实现相对更直接、更简单，详见附录 B.4。

### 结构与序列扩散对齐

尽管结构扩散的训练方法直接采样噪声水平，但我们注意到 AF3 在采样时使用了一个预定义的 200 个递减噪声水平序列。这种方法与离散时间步的概念非常相似。因此，我们考虑修改结构扩散训练，使其也仅从这 200 个噪声水平中采样噪声水平。这使我们能够为训练创建一个预定义的噪声调度表，在其中我们采样一个时间步并根据该调度表检索相应的噪声水平。相应地，我们将序列的扩散步数设置为 200，从而实现轻松对齐。由于在结构扩散的训练和推理中都使用相同的这 200 个水平，该方法转向了一种类似于经典 DDPM 离散时间训练的风格。经验结果表明，采用这种新的结构扩散方法进行微调，保留了模型准确预测复杂结构的能力。

### 离散序列扩散的实例化

扩散专门应用于 CDRs。`<X>` 自然适合作为 CDRs 中残基的吸收状态。前向过程由一个离散转移矩阵刻画，该矩阵决定了 token 变异为 `<X>` 的概率。相应的先验是一个点质量分布，位于完全由 `<X>` 组成的序列上。在 AF3 之上，我们引入了一个名为 **TokenDenoiser** 的额外模块。由于我们需要在 token 级别进行预测，因此需要 token 级别的表示。回想一下第 3.2 节，AF3 的结构扩散过程已经驱动了 token 级别的表示 $\{a_i\}$，这些表示通过 token 级别的全自注意力机制获得，并服务于结构去噪的目的。在这里，我们选择直接重用 $\{a_i\}$ 作为 TokenDenoiser 的输入。TokenDenoiser 在本文中被简单地实现为一个基本的 MLP，由连接 GELU 激活函数的 LINEAR 层组成。

接下来，我们详细说明如何在训练期间的每个采样时间步将对应的带噪序列（记为 $\{k^{\text{noisy}}_i\}$）输入到扩散模块中。如第 3.2 节所述，主干输出的 $\{s^{\text{inputs}}_i\}$ 是原始输入的基本编码，其中部分连续维度实际上是 token 的 one-hot 编码。因此，我们可以很容易地通过直接替换这些维度为 $\{k^{\text{noisy}}_i\}$ 来实现带噪序列的输入，得到 $\{s^{\text{replaced}}_i\}$。然后，$\{s^{\text{replaced}}_i\}$ 被输入到我们的结构与序列协同扩散模块中，其后续使用与 $\{s^{\text{inputs}}_i\}$ 保持一致。我们将恢复后的序列记为 $\{k^{\text{denoised}}_i\}$。请注意，除了 $\{s^{\text{inputs}}_i\}$ 外，所有符号都与特定的时间步 $t$ 相关联。然而，为了简化，我们在当前呈现的符号中省略了 $t$。

我们通过结合 AF3 的原始结构预测目标和一个专门为序列扩散设计的额外目标，对类似 AF3 的模型进行了完全微调。这个额外的目标是为了最大化去噪过程在真实 CDR 序列上的似然，从而学习 TokenDenoiser 的最优参数。序列采样遵循 D3PM-absorbing 标准方法，正如 Gruver 等人所使用的那样。为了使我们的采样能够以给定的共晶体结构为条件并重新设计 CDRs，我们采用了“替换采样”技术。该技术最初源自图像修复领域 [Lugmayr 等, 2022]，已被应用于分子和蛋白质生成任务中，以在某些基序上对采样进行条件控制 [Schneuing 等, 2024; Trippe 等, 2023]。有关训练和采样的更多细节，请参见附录 B。

![image-20251111152738553](C:/Users/wak/AppData/Roaming/Typora/typora-user-images/image-20251111152738553.png)

我们的方法在图 2 中概述。我们将 AF3 的 DiffusionModule 扩展为序列扩散，以实现结构与序列的协同设计，我们称之为 **CoDiffusionModule**，形式化描述见算法 1。它遵循一个类似于 DiffusionModule 的特征提取过程，但重用了最终获得的 token 级别表示用于序列设计。

![image-20251111152656986](C:/Users/wak/AppData/Roaming/Typora/typora-user-images/image-20251111152656986.png)

### 训练策略

与使用 token 裁剪进行训练的 AF3 模型类似，我们也实现了这一技术。我们的训练过程分为四个阶段：

*   **第一阶段**：我们仅输入抗体序列，使用最大 token 数量为 256 来容纳 VH 和 VL 序列。这允许更大的批处理大小，使模型能够快速学习设计 CDRs 并仅基于抗体序列预测抗体结构。
*   **第二阶段与第三阶段**：我们继续输入完整的 VH 和 VL 序列，同时随机从抗原表位和邻近区域选择 token，以引入抗原上下文。这两个阶段遵循附录 S2 中详述的相同裁剪方法，仅在最大 token 数量上有所不同：第二阶段为 384，第三阶段为 512。
*   **第四阶段**：我们进行全参数微调，这可能会影响模型在预训练期间学到的结构预测能力。此外，考虑到前三个阶段专注于预测抗体及附近抗原区域周围的结构，当预测远离抗体的抗原区域结构时，模型可能会表现不佳。为了解决这个问题，我们实施了一种采样策略，即有 50% 的概率从抗体周围区域选择 token，另有 50% 的概率从复合物中的任意区域选择 token。这种方法有助于模型在保留新获得的抗体协同设计能力的同时，也保持其预训练的预测复杂结构的能力。此阶段的最大 token 数量保持为 512。

### 避免数据泄露

即使在 MSA 中，抗体查询序列的 CDRs 被未知 token `<X>` 填充，MSA 结果仍包含大量与抗体序列高度一致的序列，这些序列虽然并非完全相同，但在对应于 CDRs 的位置上拥有相同的序列。这可能导致模型直接从 MSA 结果中学习预测 CDR 序列，从而导致数据泄露。为解决此问题，在训练和推理中，我们都排除了那些在查询抗体序列中与 CDRs 对应区域相似度超过阈值的序列，具体细节见附录 A.4。此处理旨在确保公平的基准测试。在实际的抗体设计中，无需进行过滤。

类似 AF3 的折叠模型天生支持 `<X>` token 的输入。对于这些未知残基，它们仅采样并去噪骨架原子坐标。与此设计一致，我们专门只为 CDRs 生成骨架原子坐标。随后，遵循 Luo 等人和 Zhu 等人的做法，我们使用 PyRosetta 进行侧链打包和结构松弛，以获得最终的全原子结构。



